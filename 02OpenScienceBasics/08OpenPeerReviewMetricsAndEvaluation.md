## <img src="/Images/Icons/peer_review.png" width="300" height="300" />

## 8. Revisión por pares abierta, métricas y evaluación 

### ¿En qué consiste?

Ser un investigador implica estar sujeto a una evaluación constante. La academia es una "economía del prestigio", donde el valor académico se basa en  la evaluación que los investigadores y sus colaboradores reciben de sus pares u otro tipo de tomadores de decisiones,  y se basa generalmente en el prestigio de su producción científico-académica  \(Blackmore and Kandiko, 2011\). Por lo tanto, en esta sección será importante distinguir entre la evaluación de los trabajos y la evaluación de los propios investigadores. Tanto los trabajos de  investigación como los investigadores se evalúan principalmente siguiendo dos métodos: la revisión por pares y las métricas, el  primero de tipo cualitativo y el segundo, cuantitativo. 

La revisión por pares se utiliza principalmente para evaluar los trabajos de investigación. Es el mecanismo formal utilizado para valorar la calidad de los manuscritos académicos \(ej. artículos de revistas, libros, postulaciones a fondos de financiamiento y comunicaciones a  congresos \) llevado a cabo por pares, cuya retroalimentación y juicio se emplean para la mejora de los trabajos y la toma de decisiones con respecto a su aceptación \(de publicación, de adjudicación de fondos de financiamiento o de su presentación\).  La revisión por pares abierta tiene significados distintos para distintas personas y comunidades, se ha definido como un término que engloba diversas formas a las que los  modelos de  evaluación por pares pueden adaptarse para estar en consonancia con  los objetivos de la Ciencia Abierta \(Ross-Hellauer, 2017\). Sus dos  principales características son  que tanto autores como recensores conocen sus identidades recíprocamente ("identidades abiertas") \(es decir, evaluación por el sistema no ciego\), y que las recensiones son  "informes abiertos", publicados junto al artículo correspondiente. Estos dos elementos pueden combinarse, aunque no es del todo necesario, y pueden complementarse con otro tipo de innovación, como la "participación abierta", donde los miembros de una determinada comunidad pueden contribuir también en el proceso de revisión, o "interacción abierta", donde se fomenta la discusión recíproca entre autor(es) y revisor(es), y "los manuscritos sometidos a una pre-evaluación abierta", en este caso se puede acceder a los manuscritos de manera inmediata antes de cualquier procedimiento formal de revisión por pares \(ya sea internamente como parte de los flujos de trabajo de una revista o externamente a través de un servidor de preprints\). 

Una vez que han pasado la revisión por pares, las publicaciones se convierten en la principal medida del trabajo de un investigador (de ahí la frase "publish or perish"). Sin embargo, evaluar la calidad de las publicaciones es difícil y subjetivo. Aunque algunas iniciativas de evaluación utilizan revisión por pares,  como el Research Excellence Framework del Reino Unido, las evaluaciones generalmente suelen estar basadas en __métricas__ tales como el número de citas generadas por las publicaciones (índice h), o por el  prestigio de la revista académica donde se publicó el artículo (cuantificado a través del Factor de Impacto de la  revista). El predominio de este tipo de métricas y la forma en que pueden distorsionar los incentivos de los investigadores, se ha denunciado y puesto de  manifiesto en los últimos años a través de algunas declaraciones como el Manifiesto de Leiden y la Declaración de San Francisco sobre la evaluación de la investigación (DORA). 

Las "Métricas Alternativas" o  [altmétricas](https://www.altmetric.com) se han convertido en  un tema relevante en el debate sobre cómo  complementar la medición basada en el número de citas, con otras mediciones del impacto de los trabajos de investigación  obtenidas de la web, que incluyan bookmarks, enlaces web, comentarios en blogs, tweets, likes, shares, aparición en la prensa,  y otros similares. Además de los posibles temas asociados a las métricas,  está el hecho de que los índices de citas los crean entidades con fines comerciales (ej., Clarivate Analytics y Elsevier), basados en sistemas propietarios, lo que puede generar reticencias respecto a su transparencia. 

## <img src="/Images/Icons/umbrella.png" width="150" height="150" />
### Argumentación

#### La revisión por pares abierta 
Desde que en el siglo XVII la Royal Society of London (1662) y la Académie Royale des Sciences de Paris (1699), asumieron el privilegio de que la ciencia se autocensurase en lugar de hacerlo la iglesia, han pasado muchos años años para que la revisión por pares se estableciera como una práctica habitual en el quehacer científico. La revisión por pares, como mecanismo formal, es mucho más joven de lo que algunos piensan. Por ejemplo, la revista Nature la puso en marcha en el año 1967. Aunque algunas encuestas muestran que los investigadores valoran la revisión por pares, también piensan que podría mejorarse. Muchas veces se quejan de que las revisiones toman mucho tiempo, de que son inconsistentes y que muchas veces fallan en la detección de  errores, y que el anonimato puede servir para emitir una opinión sesgada. La revisión por pares abierta (open peer review \(OPR\)) por tanto, busca promover mayor transparencia y participación en los procesos de revisión por pares, formales e informales. Ser un recensor es una oportunidad para los investigadores de involucrarse con una investigación puntera, crear redes académicas y redes de conocimiento, y mejorar su propia capacidad de comunicación. Es un elemento crucial del control de calidad del trabajo académico. Sin embargo, en general, los investigadores normalmente no reciben una capacitación formal sobre cómo realizar una revisión por pares. A pesar de que los investigadores confían en la evaluación por pares tradicional, las nuevas formas de revisión por pares abierta presentan nuevos desafíos y oportunidades. Como la OPR abarca un grupo de prácticas tan diverso, tanto revisores como autores deben tomar en cuenta unas nuevas consideraciones. 

## <img src="/Images/02 Open Science Basics/02_open_peer_review.png" />

La evaluación por pares, los sistemas de compensación y las métricas todavía no están en sintonía con la Ciencia Abierta. Las métricas utilizadas para evaluar la investigación \(ej. Factor de Impacto, índice-h\) no miden -  y por tanto, no premian - las prácticas de ciencia abierta. La revisión por pares abierta no está reconocida como una  actividad propiamente  "académica" en escenarios de promoción  profesional (ej. en muchos casos, los tribunales de concursos académicos no consideran como un mérito academico ni las revisiones por pares abiertas más brillantes\). Es más, muchas métricas utilizadas en los concursos de promoción - especialmente cierto tipo de bibliometrías - no son ni tan abiertas ni tan  transparentes como querría la comunidad científica. 

En estas circunstancias, en el mejor de los casos las prácticas de Ciencia Abierta son vistas como una carga adicional que no tiene reconocimiento. En el peor, son vistas como actividades que perjudican las posibilidades  de financiamiento y promoción profesional. Un [informe reciente de la Comisión Europea (2017)](https://doi.org/10.2777/75255) reconoce que existen dos aproximaciones para la implementación de la Ciencia Abierta, y las formas en que el el reconocimiento y la evaluación pueden apoyarla son: 

1. Mantener simplemente el status quo y promover una mayor apertura, mediante nuevas métricas que midan la producción científica 

2. Experimentar con nuevas prácticas  de investigación y de formas alternativas de evaluación y promover los datos abiertos, la ciencia ciudadana y una educación abierta. 

Cada vez más, las  agencias de financiamiento y las instituciones académicas están dando algunos pasos en dichas direcciones, por ejemplo, yendo más allá de evaluaciones simplemente numéricas, e incluyendo indicadores de impacto social en sus ejercicios de evaluación. Otras acciones  que las agencias de financiamiento están tomando, son p.e., permitir más tipos de trabajos de investigación \(como preprints\) en postulaciones académicas y para el financiamiento de distintos tipos de investigación \(como estudios de repetibilidad\).

## <img src="/Images/Icons/finish.png" width="150" height="150" />
### Objetivos de la capacitación

1. Reconocer los principales elementos de la revisión por pares abierta y sus potenciales ventajas y desventajas.
2. Entender las diferencias entre tipos de métricas utilizadas para evaluar la investigación y a los investigadores.
3. Involucrarse en el debate sobre la forma en que los esquemas de evaluación afectan a las maneras de funcionamiento de la academia. 

### Componentes clave
## <img src="/Images/Icons/brain.png" width="150" height="150" />
### Conocimiento
#### Revisión por pares abierta 

Las editoriales de revistas científicas como Copernicus, Frontiers, BioMed Central, eLife y F1000research, son claro ejemplo del uso de la revisión por pares abierta.

La revisión por pares abierta, en sus distintas formas, tiene las siguientes  ventajas potenciales tanto para los revisores como para los autores:

* La revisión con identidades abiertas \(no-enmascaradas\) promueve una mayor transparencia y responsabilidad de los recensores, y reduce las oportunidades de sesgos o conflictos de interés no explicitados. 

* Los informes de una evaluación abierta agregan otra capa de control de calidad, permitiendo a la comunidad científica examinar las revisiones y  los procesos de toma de decisiones.

* Las identidades e informes abiertos, conjuntamente, puede conducir a mejores recensiones, dado que la idea de tener su nombre públicamente vinculado a un trabajo o ver su revisión publicada, supone un incentivo para que los investigadores sean más rigurosos.

* Las identidades e informes abiertos permiten a los recensores ganar crédito público por su trabajo de evaluación, por tanto incentiva esta actividad y permite que la recesión pueda ser citada en otras publicaciones y reconocida en promociones profesionales. 

* La participación abierta podría superar los problemas asociados con la selección editorial de recensores \(ej. sesgos, redes cerradas, elitismo\). Especialmente para jóvenes investigadores que no han recibido solicitudes previas, estos procesos abiertos pueden representar una oportunidad para construir su reputación académica y practicar sus capacidades de revisión. 

Existen algunos inconvenientes a los que se debe poner atención, incluyendo:

* Las identidades abiertas eliminan el anonimato de los revisores \(sistema ciego)\ o la de los autores y de los revisores \(sistema doble ciego\), formas que tradicionalmente se han utilizado para contrarrestar posibles sesgos sociales \(aunque no existe evidencia  de  que este anonimato haya tenido el efecto deseado\). Por lo tanto, es  importante para los revisores que se cuestionen sus supuestos para asegurar que sus juicios se centren sólo la calidad del manuscrito, y no el estatus, historia, o afiliación de el/los autor(es)\). Los autores deben hacer lo mismo al recibir comentarios de la revisión por pares.

* Dar y recibir críticas es un proceso que conlleva inevitablemente reacciones emocionales - los autores y los revisores pueden subjetivamente acordar, o no, cómo presentar los resultados y/o que es lo que necesita mejora, enmienda o corrección. En el caso de identidades abiertas y/o informes abiertos, la transparencia podría dificultar dicho proceso. Por ello, es esencial que los revisores comuniquen de forma clara y respetuosa todos los puntos de su revisión, de manera que los autores la vean como una valiosa retroalimentación.

* La falta de anonimato de los revisores en el caso de identidades abiertas puede subvertir el proceso al desincentivar que los revisores hagan críticas desfavorables, especialmente si son para colegas de estatus superior. 

* Finalmente, debido a estos temas, los potenciales recensores podrían ser más propensos a declinar la solicitud de revisión. 

#### Métricas abiertas

La [Declaración de San Francisco sobre Evaluación de la Investigación \(DORA\)](https://sfdora.org/) recomienda distanciarse de las evaluaciones basadas solo en la reputación de las revistas científicas, y propone que se tengan en consideración los diversos  tipos de productos resultado de la actividad científica, y que se utilicen diferentes formas de métricas que complementen a las formas clásicas. DORA ha sido suscrita firmada por miles de investigadores, instituciones, editoriales y agencias de financiamiento, quienes se han comprometido a poner esto en práctica. El [Manifiesto de Leiden](http://www.leidenmanifesto.org/) provee guías sobre cómo utilizar las métricas de manera responsable.

Con respecto a las altmétricas, Priem et al. (2010) advierten de sus posibles beneficios: sus datos se recopilan de  manera más rápida que las citas; pueden medir el impacto de los resultados de la investigación distintos de las publicaciones científicas (ej. sets de datos, código, protocolos, comentarios en blogs, tweets, etc.); y pueden proveer distintas medidas de impacto de objetos digitales de forma inidividual. La inmediatez  de las altmétricas constituye una ventaja para los jóvenes investigadores, cuyo impacto de investigación puede no reflejarse en un número número significativo de citas, sin embargo su carrera depende de evaluaciones positivas.
Las altmétricas pueden ayudar a identificar una investigación nóvel e influyente y establecer  conexiones entre investigadores. Un informe reciente del grupo de expertos en altmétricas de la Comisión Europea (Directiva General para Investigación e Innovación, Comisión Europa, 2017) ha identificado los desafíos de las altmétricas: la falta de robustez y su susceptibilidad al azar; que cualquier medida cesa de ser una buena medida una vez que se convierte en un objetivo (‘Ley de Goodhart’); la relativa falta de adopción de redes sociales en algunas disciplinas y regiones geográficas; y la dependencia de entidades comerciales sobre los datos en que se basan estas métricas. 

## <img src="/Images/Icons/gears.png" width="150" height="150" />
### Competencias

Ejemplos de posibles ejercicios

* Los estudiantes trabajan en grupos de tres. Cada individuo escribe una revisión de un texto académico corto.

* Revisar una publicación de un servidor de pre-prints. 

* Uso de servicios bibliométricos o de  altmétricas  \(ej. Impactstory, Paperbuzz, Altmetric bookmarklet, Dimensions.ai\) para analizar las  métricas de un artículo, para después facilitar una breve explicación de cómo se han calculado las distintas métricas \(es más difícil de lo que piensas; supone el desafío de encontrar la documentación sobre el cálculo de las métricas incluso para aquellos servicios  más transparentes\)

## <img src="/Images/Icons/questions.png" width="150" height="150" />
### Preguntas, obstáculos e interpretaciones erróneas comunes 
Q: ¿Es justa la evaluación de la investigación?

A: La evaluación de la investigación es tan justa como lo son sus métodos y técnicas de evaluación. Las métricas y altmétricas tratan de medir la calidad de la investigación teniendo en cuenta la cantidad de los resultados de la investigación, lo que puede ser preciso, pero no necesariamente. 


## <img src="/Images/Icons/output.png" width="150" height="150" />
### Resultados de la capacitación

1. Los estudiantes serán capaces de identificar las revistas que sigan una política editorial de revisión de pares abierta.
2. Los estudiantes conocerán un rango de métricas, sus ventajas y desventajas. 

## <img src="/Images/Icons/magnifying_glass.png" width="150" height="150" />
### Lecturas adicionales 

* Peer Review the Nuts and Bolts. [A Guide for Early Career Researchers.](http://senseaboutscience.org/wp-content/uploads/2016/09/peer-review-the-nuts-and-bolts.pdf)

* [Peer Reviewers’ Openness Initiative](https://opennessinitiative.org/)

* [Open Rev. ](https://www.openrev.org/)

* [Peerage of Science](https://www.peerageofscience.org/)

* [Make Data Count](https://makedatacount.org/)

* [OpenUP Hub](https://www.openuphub.eu/review)

* [Leiden Manifesto for Research Metrics](http://www.leidenmanifesto.org/)

* [Responsible Metrics](https://responsiblemetrics.org/)

* [NISO Alternative Assessment Metrics \(Altmetrics\) Initiative](http://www.niso.org/standards-committees/altmetrics)

* [Snowball Metrics](https://www.snowballmetrics.com/)

* Directorate-General for Research and Innovation (European Commission): Evaluation of Research Careers Fully Acknowledging Open Science Practices: Rewards, Incentives and/or Recognition for Researchers Practicing Open Science. Report, 14 November 2017: [doi.org/10.2777/75255](https://doi.org/10.2777/75255)



